\documentclass[12pt]{article}

% Any percent sign marks a comment to the end of the line

% Every latex document starts with a documentclass declaration like this
% The option dvips allows for graphics, 12pt is the font size, and article
%   is the style

\usepackage[pdftex]{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\max_bottom}{max}
\usepackage{url}
\usepackage{hyperref}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{listings}

\lstset{language=python}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Sharelatex Example},
    bookmarks=true,
    pdfpagemode=FullScreen,
}


\usepackage{graphicx}
\graphicspath{ {./images/} }

% These are additional packages for "pdflatex", graphics, and to include
% hyperlinks inside a document.

\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\setlength{\topmargin}{-1.6cm}
\setlength{\leftmargin}{0.5cm}
\setlength{\rightmargin}{0.5cm}
\setlength{\textheight}{24.00cm} 
\setlength{\textwidth}{15.00cm}
\parindent 0pt
\parskip 5pt
\pagestyle{plain}

% These force using more of the margins that is the default style
\newcommand{\namelistlabel}[1]{\mbox{#1}\hfil}
\newenvironment{namelist}[1]{%1
\begin{list}{}
    {
        \let\makelabel\namelistlabel
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{1.1\labelwidth}
    }
  }{%1
\end{list}}


\begin{document}
\title{\Huge Introduction to machine learning - Homework 3}

\author{
  \textbf{Uri Kirstein}\\
  311137095 \\ sukirstn@campus.technion.ac.il
  \\ \\
  \textbf{Pavel Rastopchin}\\
  321082026 \\ pavelr@campus.technion.ac.il
  \\ \\ 
}

\maketitle


%\newpage
\section{Process and significant decisions}
\subsection{Using one model and ensemble technique}
In Homework 3, during the automatic model selection, we found that the best model for the prediction tasks is a Multi-layer perceptron. In this homework we decided to use MLP model as well, according our findings. The main problem with sklearn MLP model was a high level of abstraction which prevented from us to use more advanced techniques. Thus, in this homework we will use Keras framework to build a deep learning model. To boost our model accuracy we decided to use ensemble technique. We will train multiple MLP models, then use all of them to predict the labels. 

\subsection{Hyper parameters tuning with random search}
Based on Stanford CS231n course, lectures 6,7, we decided to use another technique for tuning the MLP parameters using random search, as this "brute force" technique might achieve better results than tuning parameters one by one. In all our cross-validation experiments we used the $accuracy$ and $f1_score$ as model performance measurement. Models which achieve more than 0.93 on both metrics will be saved and used as a part of ensemble.

\begin{figure}[h]
\centering
\includegraphics[width=.6\linewidth]{pics/random_search}
\caption{Random search of hyper-parameters}
\end{figure}

\section{MLP hyper-parameters tuning by random search}
\subsection{Dropout probability}
Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. The term "dropout" refers to dropping out units in a neural network. During the Random search we will draw a random number from 0.01 to 0.6 and this will be the dropout probability in a current model. 
\subsection{Activation functions}
We will draw randomly the activation functions for each model. The options are ReLU, Leaky ReLU and tanh, while for Leaky ReLU we also draw a slope of the negative part of the function from 0.01 to 0.5.
\subsection{Number of hidden layers and their sizes}
For current data-set, deep neural networks tend to over-fit, thus we will draw a random number of hidden layers from 0 to 5, and for each layer a number of neurons from 5 to 200 (based on our finding in HW3).
\subsection{Batch Normalization}
For each model we will decide randomly either to use or not batch normalization between the layers. 

\section{Validation results and Final predictions}
\subsection{Validation set accuracy}
\subsection{Predicted Winner}
\subsection{Predicted Votes division}



\end{document}


\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{Cross_valid_plots/SVM_C_hyper_fig_coarse.png}
  \caption{C coarse tuning}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{Cross_valid_plots/SVM_C_hyper_fig_fine.png}
  \caption{C fine tuning}
  \label{fig:sub2}
\end{subfigure}
\caption{Random search}
\label{fig:test}
\end{figure}

